# Masked Language Modeling (MLM) BERT fine-tuning configuration

model_name: "mlm_bert"

# MLM specific settings
mlm:
  mask_probability: 0.15 # Standard BERT masking
  replace_with_mask_prob: 0.8 # 80% replaced with [MASK]
  replace_with_random_prob: 0.1 # 10% replaced with random token
  # 10% kept original (implicitly)

training:
  batch_size: 16
  num_epochs: 2
  learning_rate_backbone: 2.0e-5 # Conservative for MLM fine-tuning to preserve pre-training
  learning_rate_head: 5.0e-5 # Moderate for MLM prediction head
  warmup_steps: 500
  weight_decay: 0.01
  gradient_clip: 1.0
  use_amp: true

  # MLM uses standard cross-entropy loss
  loss_type: "mlm_loss"
